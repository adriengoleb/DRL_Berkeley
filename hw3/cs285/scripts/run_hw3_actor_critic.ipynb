{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUl_qfOR8JV6"
   },
   "source": [
    "##Setup\n",
    "\n",
    "You will need to make a copy of this notebook in your Google Drive before you can edit the homework files. You can do so with **File &rarr; Save a copy in Drive**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "iizPcHAp8LnA"
   },
   "outputs": [],
   "source": [
    "#@title mount your Google Drive\n",
    "#@markdown Your work will be stored in a folder called `cs285_f2022` by default to prevent Colab instance timeouts from deleting your edits.\n",
    "\n",
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nAb10wnb8N0m"
   },
   "outputs": [],
   "source": [
    "#@title set up mount symlink\n",
    "\n",
    "DRIVE_PATH = '/content/gdrive/My\\ Drive/cs285_f2022'\n",
    "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
    "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
    "  %mkdir $DRIVE_PATH\n",
    "\n",
    "## the space in `My Drive` causes some issues,\n",
    "## make a symlink to avoid this\n",
    "SYM_PATH = '/content/cs285_f2022'\n",
    "if not os.path.exists(SYM_PATH):\n",
    "  !ln -s $DRIVE_PATH $SYM_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "gtS9-WsD8QVr"
   },
   "outputs": [],
   "source": [
    "#@title apt install requirements\n",
    "\n",
    "#@markdown Run each section with Shift+Enter\n",
    "\n",
    "#@markdown Double-click on section headers to show code.\n",
    "\n",
    "!apt update \n",
    "!apt install -y --no-install-recommends \\\n",
    "        build-essential \\\n",
    "        curl \\\n",
    "        git \\\n",
    "        gnupg2 \\\n",
    "        make \\\n",
    "        cmake \\\n",
    "        ffmpeg \\\n",
    "        swig \\\n",
    "        libz-dev \\\n",
    "        unzip \\\n",
    "        zlib1g-dev \\\n",
    "        libglfw3 \\\n",
    "        libglfw3-dev \\\n",
    "        libxrandr2 \\\n",
    "        libxinerama-dev \\\n",
    "        libxi6 \\\n",
    "        libxcursor-dev \\\n",
    "        libgl1-mesa-dev \\\n",
    "        libgl1-mesa-glx \\\n",
    "        libglew-dev \\\n",
    "        libosmesa6-dev \\\n",
    "        lsb-release \\\n",
    "        ack-grep \\\n",
    "        patchelf \\\n",
    "        wget \\\n",
    "        xpra \\\n",
    "        xserver-xorg-dev \\\n",
    "        xvfb \\\n",
    "        python-opengl \\\n",
    "        ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "VcKGekJN80NO"
   },
   "outputs": [],
   "source": [
    "#@title clone homework repo\n",
    "\n",
    "%cd $SYM_PATH\n",
    "!git clone https://github.com/berkeleydeeprlcourse/homework_fall2022.git\n",
    "%cd homework_fall2022/hw3\n",
    "%pip install -r requirements_colab.txt\n",
    "%pip install gym[box2d]==0.25.2\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title set up the Ms. Pacman and Atari environments\n",
    "\n",
    "%pip install gym[accept-rom-license]\n",
    "%pip install gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "NTiH9f9y82F_"
   },
   "outputs": [],
   "source": [
    "#@title set up virtual display\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0kPh99l87q0"
   },
   "outputs": [],
   "source": [
    "#@title test virtual display\n",
    "\n",
    "#@markdown If you see a video of a four-legged ant fumbling about, setup is complete!\n",
    "\n",
    "import gym\n",
    "from cs285.infrastructure.colab_utils import (\n",
    "    wrap_env,\n",
    "    show_video\n",
    ")\n",
    "\n",
    "env = wrap_env(gym.make(\"Ant-v4\", render_mode='rgb_array'))\n",
    "\n",
    "observation = env.reset()\n",
    "for i in range(100):\n",
    "    env.render()\n",
    "    obs, rew, term, _ = env.step(env.action_space.sample() ) \n",
    "    if term:\n",
    "      break;\n",
    "            \n",
    "env.close()\n",
    "print('Loading video...')\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QizpiHDh9Fwk"
   },
   "source": [
    "## Editing Code\n",
    "\n",
    "To edit code, click the folder icon on the left menu. Navigate to the corresponding file (`cs285_f2021/...`). Double click a file to open an editor. There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window). We sync your edits to Google Drive so that you won't lose your work in the event of an instance timeout, but you will need to re-mount your Google Drive and re-install packages with every new instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_OxQ1AZSyXC"
   },
   "source": [
    "## Run Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "id": "IzuN647wT9iJ"
   },
   "outputs": [],
   "source": [
    "#@title imports\n",
    "import os\n",
    "import time\n",
    "\n",
    "from cs285.agents.ac_agent import ACAgent\n",
    "from cs285.infrastructure.rl_trainer import RL_Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "id": "PQ9qWQu7TNb9"
   },
   "outputs": [],
   "source": [
    "#@title runtime arguments\n",
    "\n",
    "class ACArgs:\n",
    "\n",
    "  def __getitem__(self, key):\n",
    "    return getattr(self, key)\n",
    "\n",
    "  def __setitem__(self, key, val):\n",
    "    setattr(self, key, val)\n",
    "\n",
    "  def __contains__(self, key):\n",
    "    return hasattr(self, key)\n",
    "\n",
    "  env_name = 'CartPole-v0' #@param ['CartPole-v0', 'InvertedPendulum-v2', 'HalfCheetah-v2']\n",
    "  exp_name = 'q4_ac' #@param\n",
    "\n",
    "  ## PDF will tell you how to set ep_len\n",
    "  ## and discount for each environment\n",
    "  ep_len = 200 #@param {type: \"integer\"}\n",
    "\n",
    "  #@markdown batches and steps\n",
    "  batch_size = 1000 #@param {type: \"integer\"}\n",
    "  eval_batch_size =  400#@param {type: \"integer\"}\n",
    "\n",
    "  n_iter = 100 #@param {type: \"integer\"}\n",
    "  num_agent_train_steps_per_iter = 1 #@param {type: \"integer\"}\n",
    "  num_actor_updates_per_agent_update = 1 #@param {type: \"integer\"}\n",
    "  num_critic_updates_per_agent_update = 1 #@param {type: \"integer\"}\n",
    "  \n",
    "  #@markdown Actor-Critic parameters\n",
    "  discount =  0.9#@param {type: \"number\"}\n",
    "  learning_rate = 5e-3 #@param {type: \"number\"}\n",
    "  dont_standardize_advantages = False #@param {type: \"boolean\"}\n",
    "  num_target_updates = 10 #@param {type: \"integer\"}\n",
    "  num_grad_steps_per_target_update = 10 #@param {type: \"integer\"}\n",
    "  n_layers = 2 #@param {type: \"integer\"}\n",
    "  size = 64 #@param {type: \"integer\"}\n",
    "\n",
    "  #@markdown system\n",
    "  save_params = False #@param {type: \"boolean\"}\n",
    "  no_gpu = False #@param {type: \"boolean\"}\n",
    "  which_gpu = 0 #@param {type: \"integer\"}\n",
    "  seed = 1 #@param {type: \"integer\"}\n",
    "\n",
    "  #@markdown logging\n",
    "  ## default is to not log video so\n",
    "  ## that logs are small enough to be\n",
    "  ## uploaded to gradscope\n",
    "  video_log_freq =  -1#@param {type: \"integer\"}\n",
    "  scalar_log_freq = 10 #@param {type: \"integer\"}\n",
    "\n",
    "\n",
    "args = ACArgs()\n",
    "\n",
    "\n",
    "if args['video_log_freq'] > 0:\n",
    "  import warnings\n",
    "  warnings.warn(\n",
    "      '''\\nLogging videos will make eventfiles too'''\n",
    "      '''\\nlarge for the autograder. Set video_log_freq = -1'''\n",
    "      '''\\nfor the runs you intend to submit.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "id": "wqUVP5E5S1z8"
   },
   "outputs": [],
   "source": [
    "#@title Define AC trainer\n",
    "\n",
    "class AC_Trainer(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "\n",
    "        #####################\n",
    "        ## SET AGENT PARAMS\n",
    "        #####################\n",
    "\n",
    "        computation_graph_args = {\n",
    "            'n_layers': params['n_layers'],\n",
    "            'size': params['size'],\n",
    "            'learning_rate': params['learning_rate'],\n",
    "            'num_target_updates': params['num_target_updates'],\n",
    "            'num_grad_steps_per_target_update': params['num_grad_steps_per_target_update'],\n",
    "            }\n",
    "\n",
    "        estimate_advantage_args = {\n",
    "            'gamma': params['discount'],\n",
    "            'standardize_advantages': not(params['dont_standardize_advantages']),\n",
    "        }\n",
    "\n",
    "        train_args = {\n",
    "            'num_agent_train_steps_per_iter': params['num_agent_train_steps_per_iter'],\n",
    "            'num_critic_updates_per_agent_update': params['num_critic_updates_per_agent_update'],\n",
    "            'num_actor_updates_per_agent_update': params['num_actor_updates_per_agent_update'],\n",
    "        }\n",
    "\n",
    "        agent_params = {**computation_graph_args, **estimate_advantage_args, **train_args}\n",
    "\n",
    "        self.params = params\n",
    "        self.params['agent_class'] = ACAgent\n",
    "        self.params['agent_params'] = agent_params\n",
    "        self.params['train_batch_size'] = params['batch_size']\n",
    "        self.params['batch_size_initial'] = self.params['batch_size']\n",
    "        self.params['non_atari_colab_env'] = True\n",
    "\n",
    "        ################\n",
    "        ## RL TRAINER\n",
    "        ################\n",
    "\n",
    "        self.rl_trainer = RL_Trainer(self.params)\n",
    "\n",
    "    def run_training_loop(self):\n",
    "\n",
    "        self.rl_trainer.run_training_loop(\n",
    "            self.params['n_iter'],\n",
    "            collect_policy = self.rl_trainer.agent.actor,\n",
    "            eval_policy = self.rl_trainer.agent.actor,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xuNw8N1jTg1p"
   },
   "outputs": [],
   "source": [
    "#@title create directories for logging\n",
    "\n",
    "data_path = '''/content/cs285_f2021/''' \\\n",
    "        '''homework_fall2021/hw3/data'''\n",
    "\n",
    "if not (os.path.exists(data_path)):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "logdir = args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "logdir = os.path.join(data_path, logdir)\n",
    "args['logdir'] = logdir\n",
    "if not(os.path.exists(logdir)):\n",
    "    os.makedirs(logdir)\n",
    "\n",
    "print(\"LOGGING TO: \", logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_IGogH9YTt1y"
   },
   "outputs": [],
   "source": [
    "#@title run training\n",
    "trainer = AC_Trainer(args)\n",
    "trainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjhrgXnUTzyi"
   },
   "outputs": [],
   "source": [
    "#@markdown You can visualize your runs with tensorboard from within the notebook\n",
    "\n",
    "## requires tensorflow==2.3.0\n",
    "# %load_ext tensorboard\n",
    "%tensorboard --logdir /content/cs285_f2021/homework_fall2021/hw3/data/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Sanity check with cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python cs285/scripts/run_hw3_actor_critic.py --env_name CartPole-v0 -n 100 -b 1000 --exp_name q4_ac_1_1 -ntu 1 -ngsptu 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python cs285/scripts/run_hw3_actor_critic.py --env_name CartPole-v0 -n 100 -b 1000 --exp_name q4_100_1 -ntu 100 -ngsptu 1 -dir q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python cs285/scripts/run_hw3_actor_critic.py --env_name CartPole-v0 -n 100 -b 1000 --exp_name q4_1_100 -ntu 1 -ngsptu 100 -dir q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python cs285/scripts/run_hw3_actor_critic.py --env_name CartPole-v0 -n 100 -b 1000 --exp_name q4_10_10 -ntu 10 -ngsptu 10 -dir data/q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import more_itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import numpy as np\n",
    "\n",
    "expname = \"CartPole-v0\"\n",
    "ax = plt.figure().gca()\n",
    "dir = \"data/q4\"\n",
    "iters = list(range(10))\n",
    "steps = iters\n",
    "\n",
    "\n",
    "for FolderName in os.listdir(dir):\n",
    "  for FileName in os.listdir(dir+\"/\"+FolderName):\n",
    "    ea = EventAccumulator(dir+\"/\"+FolderName+\"/\"+FileName).Reload()\n",
    "    AvgReturnList = [e.value for e in ea.Scalars(\"Eval_AverageReturn\")]\n",
    "    StdReturnList = [e.value for e in ea.Scalars(\"Eval_StdReturn\")]\n",
    "    avg_return_steps, std_return_steps = AvgReturnList, StdReturnList\n",
    "    ListeUnderScore = list(more_itertools.locate(FolderName, lambda x: x == \"_\"))\n",
    "    Label = FolderName[3:8]\n",
    "    plt.errorbar(steps, avg_return_steps, label=Label)\n",
    "    plt.fill_between(steps, np.array(avg_return_steps) + np.array(std_return_steps) ,np.array(avg_return_steps) - np.array(std_return_steps), alpha=0.5)\n",
    "\n",
    "\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Average reward\")\n",
    "plt.title(expname)\n",
    "plt.legend()\n",
    "plt.savefig(\"Learning curves for small batch experiment\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Run actor-critic with more difficult tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python cs285/scripts/run_hw3_actor_critic.py --env_name InvertedPendulum-v2 --ep_len 100 --discount 0.95 -n 100 -l 2 -s 64 -b 5000 -lr 0.01 --exp_name q5_10_10 -ntu 10 -ngsptu 10 -dir q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python cs285/scripts/run_hw3_actor_critic.py --env_name HalfCheetah-v2 --ep_len 150 --discount 0.90 --scalar_log_freq 1 -n 150 -l 2 -s 32 -b 30000 -eb 1500 -lr 0.02 --exp_name q5_10_10 -ntu 10 -ngsptu 10 -dir q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import more_itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import numpy as np\n",
    "\n",
    "expname = \"CartPole-v0\"\n",
    "ax = plt.figure().gca()\n",
    "dir = \"data/q5\"\n",
    "iters = list(range(10))\n",
    "steps = iters\n",
    "\n",
    "\n",
    "for FolderName in os.listdir(dir):\n",
    "  for FileName in os.listdir(dir+\"/\"+FolderName):\n",
    "    ea = EventAccumulator(dir+\"/\"+FolderName+\"/\"+FileName).Reload()\n",
    "    AvgReturnList = [e.value for e in ea.Scalars(\"Eval_AverageReturn\")]\n",
    "    StdReturnList = [e.value for e in ea.Scalars(\"Eval_StdReturn\")]\n",
    "    avg_return_steps, std_return_steps = AvgReturnList, StdReturnList\n",
    "    print(len(avg_return_steps))\n",
    "    ListeUnderScore = list(more_itertools.locate(FolderName, lambda x: x == \"_\"))\n",
    "    Label = FolderName[3:8]\n",
    "    plt.errorbar(steps, avg_return_steps, label=Label)\n",
    "    plt.fill_between(steps, np.array(avg_return_steps) + np.array(std_return_steps) ,np.array(avg_return_steps) - np.array(std_return_steps), alpha=0.5)\n",
    "    break\n",
    "  break\n",
    "\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Average reward\")\n",
    "plt.title(expname)\n",
    "plt.legend()\n",
    "plt.savefig(\"Learning curves for small batch experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import more_itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import numpy as np\n",
    "\n",
    "expname = \"CartPole-v0\"\n",
    "ax = plt.figure().gca()\n",
    "dir = \"data/q5\"\n",
    "iters = list(range(150))\n",
    "steps = iters\n",
    "done = False\n",
    "\n",
    "for FolderName in os.listdir(dir):\n",
    "  for FileName in os.listdir(dir+\"/\"+FolderName):\n",
    "    if done:\n",
    "      ea = EventAccumulator(dir+\"/\"+FolderName+\"/\"+FileName).Reload()\n",
    "      AvgReturnList = [e.value for e in ea.Scalars(\"Eval_AverageReturn\")]\n",
    "      StdReturnList = [e.value for e in ea.Scalars(\"Eval_StdReturn\")]\n",
    "      avg_return_steps, std_return_steps = AvgReturnList, StdReturnList\n",
    "      print(len(avg_return_steps))\n",
    "      ListeUnderScore = list(more_itertools.locate(FolderName, lambda x: x == \"_\"))\n",
    "      Label = FolderName[3:8]\n",
    "      plt.errorbar(steps, avg_return_steps, label=Label)\n",
    "      plt.fill_between(steps, np.array(avg_return_steps) + np.array(std_return_steps) ,np.array(avg_return_steps) - np.array(std_return_steps), alpha=0.5)\n",
    "    else:\n",
    "      done = True\n",
    "\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Average reward\")\n",
    "plt.title(expname)\n",
    "plt.legend()\n",
    "plt.savefig(\"Learning curves for small batch experiment\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "run_hw3_actor_critic.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bfb5a87c35bcd43997975c71900d67fd988e3d78f446c88335b459b4ff2da931"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
